{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils import get_classes\n",
    "from utils.utils_map import get_coco_map, get_map\n",
    "from yolo import YOLO\n",
    "\n",
    "\n",
    "'''\n",
    "Recall和Precision不像AP是一个面积的概念，因此在门限值（Confidence）不同时，网络的Recall和Precision值是不同的。\n",
    "默认情况下，本代码计算的Recall和Precision代表的是当门限值（Confidence）为0.5时，所对应的Recall和Precision值。\n",
    "\n",
    "受到mAP计算原理的限制，网络在计算mAP时需要获得近乎所有的预测框，这样才可以计算不同门限条件下的Recall和Precision值\n",
    "因此，本代码获得的map_out/detection-results/里面的txt的框的数量一般会比直接predict多一些，目的是列出所有可能的预测框\n",
    "'''\n",
    "#------------------------------------------------------------------------------------------------------------------#\n",
    "#   map_mode用于指定该文件运行时计算的内容\n",
    "#   map_mode为0代表整个map计算流程，包括获得预测结果、获得真实框、计算VOC_map。\n",
    "#   map_mode为1代表仅仅获得预测结果。\n",
    "#   map_mode为2代表仅仅获得真实框。\n",
    "#   map_mode为3代表仅仅计算VOC_map。\n",
    "#   map_mode为4代表利用COCO工具箱计算当前数据集的0.50:0.95map。需要获得预测结果、获得真实框后并安装pycocotools才行\n",
    "#-------------------------------------------------------------------------------------------------------------------#\n",
    "map_mode        = 0\n",
    "#--------------------------------------------------------------------------------------#\n",
    "#   此处的classes_path用于指定需要测量VOC_map的类别\n",
    "#   一般情况下与训练和预测所用的classes_path一致即可\n",
    "#--------------------------------------------------------------------------------------#\n",
    "classes_path    = 'model_data/vehicle_classes.txt'\n",
    "#--------------------------------------------------------------------------------------#\n",
    "#   MINOVERLAP用于指定想要获得的mAP0.x，mAP0.x的意义是什么请同学们百度一下。\n",
    "#   比如计算mAP0.75，可以设定MINOVERLAP = 0.75。\n",
    "#\n",
    "#   当某一预测框与真实框重合度大于MINOVERLAP时，该预测框被认为是正样本，否则为负样本。\n",
    "#   因此MINOVERLAP的值越大，预测框要预测的越准确才能被认为是正样本，此时算出来的mAP值越低，\n",
    "#--------------------------------------------------------------------------------------#\n",
    "MINOVERLAP      = 0.5\n",
    "#--------------------------------------------------------------------------------------#\n",
    "#   受到mAP计算原理的限制，网络在计算mAP时需要获得近乎所有的预测框，这样才可以计算mAP\n",
    "#   因此，confidence的值应当设置的尽量小进而获得全部可能的预测框。\n",
    "#   \n",
    "#   该值一般不调整。因为计算mAP需要获得近乎所有的预测框，此处的confidence不能随便更改。\n",
    "#   想要获得不同门限值下的Recall和Precision值，请修改下方的score_threhold。\n",
    "#--------------------------------------------------------------------------------------#\n",
    "confidence      = 0.8\n",
    "#--------------------------------------------------------------------------------------#\n",
    "#   预测时使用到的非极大抑制值的大小，越大表示非极大抑制越不严格。\n",
    "#   \n",
    "#   该值一般不调整。\n",
    "#--------------------------------------------------------------------------------------#\n",
    "nms_iou         = 0.5\n",
    "#---------------------------------------------------------------------------------------------------------------#\n",
    "#   Recall和Precision不像AP是一个面积的概念，因此在门限值不同时，网络的Recall和Precision值是不同的。\n",
    "#   \n",
    "#   默认情况下，本代码计算的Recall和Precision代表的是当门限值为0.5（此处定义为score_threhold）时所对应的Recall和Precision值。\n",
    "#   因为计算mAP需要获得近乎所有的预测框，上面定义的confidence不能随便更改。\n",
    "#   这里专门定义一个score_threhold用于代表门限值，进而在计算mAP时找到门限值对应的Recall和Precision值。\n",
    "#---------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "score_threhold  = 0\n",
    "#-------------------------------------------------------#\n",
    "#   map_vis用于指定是否开启VOC_map计算的可视化\n",
    "#-------------------------------------------------------#\n",
    "map_vis         = False\n",
    "\n",
    "#-------------------------------------------------------#\n",
    "#   结果输出的文件夹，默认为map_out\n",
    "#-------------------------------------------------------#\n",
    "map_out_path    = f'test_result'\n",
    "\n",
    "model_path = \"logs/last_epoch_weights.pth\"\n",
    "\n",
    "phi = \"l\"\n",
    "\n",
    "dataset = \"Dataset1.0\"\n",
    "\n",
    "image_ids = open( f\"{dataset}/test_split.txt\").read().strip().split()\n",
    "\n",
    "if not os.path.exists(map_out_path):\n",
    "    os.makedirs(map_out_path)\n",
    "if not os.path.exists(os.path.join(map_out_path, 'ground-truth')):\n",
    "    os.makedirs(os.path.join(map_out_path, 'ground-truth'))\n",
    "if not os.path.exists(os.path.join(map_out_path, 'detection-results')):\n",
    "    os.makedirs(os.path.join(map_out_path, 'detection-results'))\n",
    "if not os.path.exists(os.path.join(map_out_path, 'images-optional')):\n",
    "    os.makedirs(os.path.join(map_out_path, 'images-optional'))\n",
    "\n",
    "class_names, _ = get_classes(classes_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model.\n",
      "21\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/39979 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "logs/last_epoch_weights.pth model, and classes loaded.\n",
      "Configurations:\n",
      "----------------------------------------------------------------------\n",
      "|                     keys |                                   values|\n",
      "----------------------------------------------------------------------\n",
      "|               model_path |              logs/last_epoch_weights.pth|\n",
      "|             classes_path |           model_data/vehicle_classes.txt|\n",
      "|             anchors_path |              model_data/yolo_anchors.txt|\n",
      "|             anchors_mask |        [[6, 7, 8], [3, 4, 5], [0, 1, 2]]|\n",
      "|              input_shape |                               [640, 640]|\n",
      "|                      phi |                                        l|\n",
      "|               confidence |                                      0.8|\n",
      "|                  nms_iou |                                      0.5|\n",
      "|          letterbox_image |                                     True|\n",
      "|                     cuda |                                     True|\n",
      "----------------------------------------------------------------------\n",
      "Load model done.\n",
      "Get predict result.\n",
      "Dataset1.0/images/C009_001565.jpg\n",
      "Dataset1.0/images/C009_001565.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/39979 [00:00<3:06:15,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1.0/images/C009_001565.jpg\n",
      "815,421,965,562,0\n",
      "815,421,965,562,0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/zhengao/00workspace/CAP5495SmartCitiesProject/YOLOv7Deepsort/815,421,965,562,0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_id)\n\u001b[0;32m---> 12\u001b[0m image       \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m map_vis:\n\u001b[1;32m     14\u001b[0m     image\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(map_out_path,  image_id[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m:]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLOv7Env/lib/python3.8/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/zhengao/00workspace/CAP5495SmartCitiesProject/YOLOv7Deepsort/815,421,965,562,0'"
     ]
    }
   ],
   "source": [
    "if map_mode == 0 or map_mode == 1:\n",
    "    print(\"Load model.\")\n",
    "    yolo = YOLO(confidence = confidence, nms_iou = nms_iou, model_path = model_path, phi = phi, classes_path = classes_path)\n",
    "    \n",
    "    print(\"Load model done.\")\n",
    "\n",
    "    print(\"Get predict result.\")\n",
    "    for image_id in tqdm(image_ids):\n",
    "        image_path  =  image_id\n",
    "        print(image_path)\n",
    "        print(image_id)\n",
    "        image       = Image.open(image_id)\n",
    "        if map_vis:\n",
    "            image.save(os.path.join(map_out_path,  image_id[-15:].replace(\".jpg\", \".txt\")))\n",
    "            print(image_id)\n",
    "        yolo.get_map_txt(image_id[-15:].replace(\".jpg\", \"\"), image, class_names, map_out_path)\n",
    "        print(image_id)\n",
    "    print(\"Get predict result done.\")\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "if map_mode == 0 or map_mode == 2:\n",
    "    print(\"Get ground truth result.\")\n",
    "    for image_id in tqdm(image_ids):\n",
    "        with open(os.path.join(map_out_path, \"ground-truth/\"+image_id+\".txt\"), \"w\") as new_f:\n",
    "            root = ET.parse(f\"{dataset}/testlabel/\"+image_id+\".xml\").getroot()\n",
    "            for obj in root.findall('object'):\n",
    "                difficult_flag = False\n",
    "                if obj.find('difficult')!=None:\n",
    "                    difficult = obj.find('difficult').text\n",
    "                    if int(difficult)==1:\n",
    "                        difficult_flag = True\n",
    "                obj_name = obj.find('name').text\n",
    "                if obj_name not in class_names:\n",
    "                    continue\n",
    "                bndbox  = obj.find('bndbox')\n",
    "                left    = bndbox.find('xmin').text\n",
    "                top     = bndbox.find('ymin').text\n",
    "                right   = bndbox.find('xmax').text\n",
    "                bottom  = bndbox.find('ymax').text\n",
    "\n",
    "                if difficult_flag:\n",
    "                    new_f.write(\"%s %s %s %s %s difficult\\n\" % (obj_name, left, top, right, bottom))\n",
    "                else:\n",
    "                    new_f.write(\"%s %s %s %s %s\\n\" % (obj_name, left, top, right, bottom))\n",
    "    print(\"Get ground truth result done.\")\n",
    "\n",
    "if map_mode == 0 or map_mode == 3:\n",
    "    print(\"Get map.\")\n",
    "    get_map(MINOVERLAP, True, score_threhold = score_threhold, path = map_out_path)\n",
    "    print(\"Get map done.\")\n",
    "\n",
    "if map_mode == 4:\n",
    "    print(\"Get map.\")\n",
    "    get_coco_map(class_names = class_names, path = map_out_path)\n",
    "    print(\"Get map done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据\n",
    "classes = ['Cheese', 'Chicken', 'Spinach', 'bsu', 'ecl', 'lm']\n",
    "data = np.array([\n",
    "    [365, 96, 0, 0, 0, 0],\n",
    "    [0, 174, 20, 0, 0, 0],\n",
    "    [0, 0, 51, 7, 0, 0],\n",
    "    [0, 0, 0, 97, 0, 0],\n",
    "    [0, 0, 0, 0, 276, 0],\n",
    "    [0, 0, 0, 0, 0, 248]\n",
    "])\n",
    "\n",
    "# 绘制混淆矩阵\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(data, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# 在每个单元格中添加数值\n",
    "thresh = data.max() / 2.\n",
    "for i, j in np.ndindex(data.shape):\n",
    "    plt.text(j, i, data[i, j], horizontalalignment=\"center\", color=\"white\" if data[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YOLOv7Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
